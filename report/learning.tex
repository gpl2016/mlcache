\section{Learning Model}

We apply a simple linear model to keep track of our scores. This provides us with two benefits: the implementation is simple, and the overhead introduced by the constant processing of the scores is minimized while maintaining relevance. We use a multi-armed bandit approach for our model, using the UCB1 algorithm. The possible actions to choose from are the current k pages in the cache, and we maximize according to the following formula at time step $t>0$: $$\max_{1 \leq j \leq k} \bar{x}_{j} + \sqrt{\frac{2\log(t)}{n_{j}}}$$  where $\bar{x}_{j}$ is the average reward obtained from action $j$, $n_{j}$ is the number of times action j has been played.

Due to constraints for numerical calculations in kernel space, and the fact that we want to evict the worst page in the cache, we have modified the algorithm to better fit our scenario. Thus, our implementation evicts according to the following formula:  $$\max_{1 \leq j \leq k} -\bar{x}_{j} - isqrt(\frac{2S^{2}ilog(S^{2}t)}{n_{j}})$$ where isqrt and ilog are the integer versions of the sqrt and log functions. For example, $isqrt(x)$ is the biggest integer k such that $k^{2}\leq x$, and similarly with $ilog(x)$. This is due to floating point operations not being readily available in the Linux kernel. $S$ is a scaling factor chosen so as to not lose too much precision in our operations. In fact, we scale every single operation by this factor: time steps are multiplied by the scaling factor, as well as rewards and the number of times an action has been chosen. In our implementation we chose $S=100$, as this gives us about 4 decimal points of precision in the sqrt operation. Unfortunately, the nature of $\log$ makes even this factor relatively low, thus precision is lost in this operation. The final modification is taking the negative of the previous sum, as we represent better pages by lower scores, since we evict the maximum of the scores.