\section{Limitations and Future Work}
Our current prototype has several limitations. With regards to the learning model, ideally we would have a persistent score per page throughout several executions of a program. This would provide greater accuracy. Unfortunately, due to the early state of the project, we haven't been able to achieve this, along with an initial training phase which would also bolster the accuracy of the scores. Because of this lack of persistent scores throughout all pages, every time a new page is introduced to the cache, we assign an average score across all pages, which may make the page look better or worse than it actually is to the algorithm. Memory limitations also add to this issue, as the pages referenced by a file are in a constant state of flux, and we were experiencing heavy memory usage when keeping track of all scores.

Another limitation the model has is, of course, the lack of precision mentioned before from not being able to use floating point operations, although we expect this to not be too much of an issue, as the sqrt operation is actually quite precise.

Although we explicitly chose the model to not bring unnecessary overhead to the kernel, our prototype is built on top of the Linux LRU implementation, and as such, introduces much overhead to the current system during eviction: for each page to evict, we scan through a linked list to find the maximum score. Ideally, we would have a different data structure to keep track of scores, such as a priority queue. This would greatly decrease the runtime of system overall.  